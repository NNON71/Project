# CLIP Backbone Training Configuration (Stage 1)

experiment:
  name: "dinov2_clip_backbone"
  seed: 42
  device: "cuda"

model:
  projection_dim: 768
  image_encoder: "openai/clip-vit-base-patch32"
  text_encoder: "clicknext/phayathaibert"
  freeze_vision_encoder: true
  freeze_text_encoder: true

dataset:
  # train_images: "data/clip_pretraining/images/train"
  # train_captions: "data/clip_pretraining/captions/train_captions.json"
  # val_images: "data/clip_pretraining/images/val"
  # val_captions: "data/clip_pretraining/captions/val_captions.json"
  streaming: false
  image_size: 518
  num_workers: 4

training:
  epochs: 3
  batch_size: 64
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_epochs: 1
  grad_clip_norm: 1.0
  mixed_precision: true
  save_interval: 1

wandb:
  enabled: true
  project: "thai-owlvit"
  entity: "NNON71"
  run_name: "dinov2_clip_backbone_training_v1"
  log_interval: 10

checkpoint:
  save_dir: "checkpoints/clip_resnet/clip"
  save_best: true
  save_last: true